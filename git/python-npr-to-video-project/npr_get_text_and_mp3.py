#!/usr/bin/env python3
"""
NPR Article Scraper - L·∫•y transcript v√† audio t·ª´ c√°c b√†i vi·∫øt NPR
T√°c gi·∫£: Script t·ª± ƒë·ªông
Ng√†y t·∫°o: 2026-01-01
"""

import os
import sys
import json
import re
from datetime import datetime
from pathlib import Path
import requests
from bs4 import BeautifulSoup


# L∆∞u input history v√†o trong file n√†y d∆∞·ªõi d·∫°ng comment
# HISTORY_START
LAST_INPUTS = {
    "url": ""
}
# HISTORY_END


def get_last_input(key):
    """L·∫•y input l·∫ßn tr∆∞·ªõc t·ª´ LAST_INPUTS"""
    return LAST_INPUTS.get(key, "")


def save_last_input(key, value):
    """L∆∞u input v√†o file script"""
    global LAST_INPUTS
    LAST_INPUTS[key] = value
    
    # ƒê·ªçc n·ªôi dung file hi·ªán t·∫°i
    with open(__file__, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # T√¨m v√† thay th·∫ø ph·∫ßn LAST_INPUTS
    pattern = r'(# HISTORY_START\nLAST_INPUTS = )({[^}]*})(# HISTORY_END)'
    new_dict = json.dumps(LAST_INPUTS, ensure_ascii=False, indent=4)
    replacement = f'\\1{new_dict}\n\\3'
    
    new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    # Ghi l·∫°i file
    with open(__file__, 'w', encoding='utf-8') as f:
        f.write(new_content)


def get_user_input():
    """L·∫•y input t·ª´ user"""
    print("=" * 70)
    print("NPR ARTICLE SCRAPER - L·∫•y transcript v√† audio t·ª´ NPR")
    print("=" * 70)
    print()
    
    # L·∫•y URL
    last_url = get_last_input("url")
    if last_url:
        print(f"URL l·∫ßn tr∆∞·ªõc: {last_url}")
    
    url = input("Nh·∫≠p URL c·ªßa b√†i vi·∫øt NPR (ho·∫∑c Enter ƒë·ªÉ d√πng URL l·∫ßn tr∆∞·ªõc): ").strip()
    
    if not url and last_url:
        url = last_url
        print(f"S·ª≠ d·ª•ng URL: {url}")
    elif not url:
        print("‚ùå URL kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!")
        sys.exit(1)
    
    # Validate URL
    if not url.startswith("http"):
        print("‚ùå URL kh√¥ng h·ª£p l·ªá! URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http ho·∫∑c https")
        sys.exit(1)
    
    # L∆∞u input
    save_last_input("url", url)
    
    return url


def create_output_folder():
    """T·∫°o folder output theo format ng√†y gi·ªù"""
    now = datetime.now()
    folder_name = now.strftime("%Y_%m_%d__%H_%M")
    folder_path = Path(folder_name)
    folder_path.mkdir(exist_ok=True)
    print(f"‚úÖ ƒê√£ t·∫°o folder: {folder_name}")
    return folder_path


def download_webpage(url, folder_path):
    """Download trang web"""
    print(f"\nüì• ƒêang t·∫£i trang web t·ª´: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # L∆∞u HTML
        html_file = folder_path / "page.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        print(f"‚úÖ ƒê√£ l∆∞u trang web: {html_file}")
        return response.text
    
    except requests.exceptions.RequestException as e:
        print(f"‚ùå L·ªói khi t·∫£i trang web: {e}")
        sys.exit(1)


def extract_title_and_transcript(html_content):
    """Tr√≠ch xu·∫•t title v√† transcript t·ª´ HTML"""
    print("\nüìù ƒêang tr√≠ch xu·∫•t title v√† transcript...")
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # T√¨m title - c√≥ nhi·ªÅu c√°ch
    title = None
    
    # C√°ch 1: T√¨m trong h1 class="transcript"
    title_h1 = soup.find('h1', class_='transcript')
    if title_h1:
        title = title_h1.get_text(strip=True)
        # Lo·∫°i b·ªè k√Ω t·ª± < v√† kho·∫£ng tr·∫Øng ƒë·∫ßu
        title = re.sub(r'^<\s*', '', title)
    
    # C√°ch 2: T√¨m trong meta tag
    if not title:
        title_meta = soup.find('meta', property='og:title')
        if title_meta:
            title = title_meta.get('content', '').strip()
    
    # C√°ch 3: T√¨m trong input hidden
    if not title:
        title_input = soup.find('input', id=lambda x: x and x.startswith('title'))
        if title_input:
            title = title_input.get('value', '').strip()
    
    # T√¨m transcript - t√¨m ph·∫ßn c√≥ class ch·ª©a "transcript"
    transcript = ""
    
    # T√¨m div c√≥ class="transcript"
    transcript_div = soup.find('div', class_='transcript')
    
    if transcript_div:
        # L·∫•y t·∫•t c·∫£ p tags trong div n√†y
        paragraphs = transcript_div.find_all('p', recursive=False)
        transcript_parts = []
        
        for p in paragraphs:
            text = p.get_text(strip=True)
            # B·ªè qua c√°c ƒëo·∫°n r·ªóng
            if text and text not in transcript_parts:
                transcript_parts.append(text)
        
        transcript = '\n\n'.join(transcript_parts)
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ c√°ch kh√°c
    if not transcript:
        article = soup.find('article')
        if article:
            # T√¨m t·∫•t c·∫£ p tags
            paragraphs = article.find_all('p')
            transcript_parts = []
            seen_texts = set()  # ƒê·ªÉ tr√°nh tr√πng l·∫∑p
            in_transcript = False
            
            for p in paragraphs:
                text = p.get_text(strip=True)
                
                # B·∫Øt ƒë·∫ßu khi g·∫∑p HOST: ho·∫∑c BYLINE:
                if ('HOST:' in text or 'BYLINE:' in text) and not in_transcript:
                    in_transcript = True
                
                if in_transcript:
                    # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ trong set (tr√°nh tr√πng l·∫∑p)
                    if text and text not in seen_texts:
                        # B·ªè qua c√°c disclaimer
                        if 'Copyright ¬©' in text or 'NPR.  All rights reserved' in text:
                            break
                        
                        transcript_parts.append(text)
                        seen_texts.add(text)
                    
                    # K·∫øt th√∫c khi g·∫∑p "Thank you" v√† ƒë√£ c√≥ ƒë·ªß n·ªôi dung
                    if 'Thank you' in text and len(transcript_parts) > 10:
                        break
            
            transcript = '\n\n'.join(transcript_parts)
    
    if not title:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y title")
        title = "No title found"
    else:
        print(f"‚úÖ Title: {title}")
    
    if not transcript:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y transcript")
    else:
        print(f"‚úÖ Transcript: {len(transcript)} k√Ω t·ª±")
    
    return title, transcript


def download_audio(html_content, folder_path):
    """T√¨m v√† download file audio MP3"""
    print("\nüéµ ƒêang t√¨m v√† t·∫£i audio...")
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # T√¨m audio URL
    audio_url = None
    
    # C√°ch 1: T√¨m trong link download
    download_link = soup.find('a', href=re.compile(r'\.mp3'))
    if download_link:
        audio_url = download_link.get('href')
    
    # C√°ch 2: T√¨m trong data attribute
    if not audio_url:
        audio_data = soup.find(attrs={'data-audio': True})
        if audio_data:
            # Parse JSON data
            try:
                audio_info = json.loads(audio_data.get('data-audio', '{}'))
                audio_url = audio_info.get('audioUrl') or audio_info.get('url')
            except:
                pass
    
    # C√°ch 3: T√¨m b·∫±ng regex trong to√†n b·ªô HTML
    if not audio_url:
        mp3_pattern = r'https?://[^\s<>"]+?\.mp3[^\s<>"]*'
        matches = re.findall(mp3_pattern, html_content)
        if matches:
            # L·∫•y URL ƒë·∫ßu ti√™n c√≥ ch·ª©a "npr" ho·∫∑c "ondemand"
            for match in matches:
                if 'npr' in match.lower() or 'ondemand' in match.lower():
                    audio_url = match
                    # Clean URL - lo·∫°i b·ªè c√°c k√Ω t·ª± escape
                    audio_url = audio_url.replace('\\', '')
                    break
    
    if not audio_url:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y audio URL")
        return None
    
    print(f"üîó Audio URL: {audio_url}")
    
    # Download audio
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(audio_url, headers=headers, timeout=60, stream=True)
        response.raise_for_status()
        
        audio_file = folder_path / "audio.mp3"
        
        # Download v·ªõi progress
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(audio_file, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\r‚è≥ ƒêang t·∫£i: {percent:.1f}%", end='')
        
        print(f"\n‚úÖ ƒê√£ l∆∞u audio: {audio_file} ({downloaded / 1024 / 1024:.2f} MB)")
        return audio_file
    
    except requests.exceptions.RequestException as e:
        print(f"\n‚ùå L·ªói khi t·∫£i audio: {e}")
        return None


def save_transcript(title, transcript, folder_path):
    """L∆∞u title v√† transcript v√†o file txt"""
    print("\nüíæ ƒêang l∆∞u transcript...")
    
    txt_file = folder_path / "transcript.txt"
    
    # Lo·∫°i b·ªè ph·∫ßn copyright disclaimer
    copyright_markers = [
        'Copyright ¬©',
        'NPR.  All rights reserved',
        'Accuracy and availability of NPR transcripts',
        'The authoritative record of NPR'
    ]
    
    # T√¨m v·ªã tr√≠ c·ªßa copyright
    copyright_index = -1
    for marker in copyright_markers:
        idx = transcript.find(marker)
        if idx != -1:
            copyright_index = idx
            break
    
    # C·∫Øt b·ªè ph·∫ßn copyright
    if copyright_index != -1:
        transcript = transcript[:copyright_index].strip()
    
    # Format l·∫°i: xu·ªëng d√≤ng tr∆∞·ªõc m·ªói speaker (CH·ªÆ HOA + : + kho·∫£ng c√°ch)
    # Pattern: t√¨m c√°c ch·ªØ c√°i in hoa + d·∫•u: + space (v√≠ d·ª•: "TREISMAN: ")
    # Th√™m \n\n tr∆∞·ªõc speaker name n·∫øu kh√¥ng c√≥ newline
    transcript = re.sub(r'([a-z.,!?;)\]])([A-Z]{2,}:)\s', r'\1\n\n\2 ', transcript)
    
    # Lo·∫°i b·ªè t·∫•t c·∫£ c√°c d√≤ng tr·ªëng li√™n ti·∫øp (ch·ªâ gi·ªØ 1 d√≤ng tr·ªëng)
    transcript = re.sub(r'\n\s*\n', '\n\n', transcript)
    
    # Lo·∫°i b·ªè 2+ d√≤ng tr·ªëng li√™n ti·∫øp
    transcript = re.sub(r'\n{3,}', '\n\n', transcript)
    transcript = transcript.strip()
    
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(f"TITLE: {title}\n")
        f.write("=" * 70 + "\n")
        f.write(transcript)
    
    print(f"‚úÖ ƒê√£ l∆∞u transcript: {txt_file}")
    return txt_file


def main():
    """Main function"""
    try:
        # L·∫•y input
        url = get_user_input()
        
        # T·∫°o folder output
        folder_path = create_output_folder()
        
        # Download trang web
        html_content = download_webpage(url, folder_path)
        
        # Tr√≠ch xu·∫•t title v√† transcript
        title, transcript = extract_title_and_transcript(html_content)
        
        # L∆∞u transcript
        save_transcript(title, transcript, folder_path)
        
        # Download audio
        download_audio(html_content, folder_path)
        
        print("\n" + "=" * 70)
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print(f"üìÅ T·∫•t c·∫£ file ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {folder_path}")
        print("=" * 70)
    
    except KeyboardInterrupt:
        print("\n\n‚ùå ƒê√£ h·ªßy b·ªüi ng∆∞·ªùi d√πng")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
